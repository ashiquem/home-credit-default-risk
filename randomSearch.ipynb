{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgbm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocess data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "data = pd.read_csv('data/application_train.csv')\n",
    "#select only numerical features\n",
    "data = data.select_dtypes('number')\n",
    "#sample a small portion\n",
    "data = data.sample(n=20000,random_state =50)\n",
    "#extract labels\n",
    "labels = np.array(data['TARGET'])\n",
    "#drop ids \n",
    "data.drop(columns=['SK_ID_CURR','TARGET'],inplace=True)\n",
    "#split into training and testing set\n",
    "x_train, x_test, y_train, y_test = train_test_split(data,labels, test_size = 5000,random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to light gbm datasets\n",
    "train_set = lgbm.Dataset(x_train,y_train)\n",
    "test_set = lgbm.Dataset(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Specify Search Domain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting_type': 'gbdt',\n",
       " 'class_weight': None,\n",
       " 'colsample_bytree': 1.0,\n",
       " 'learning_rate': 0.1,\n",
       " 'max_depth': -1,\n",
       " 'min_child_samples': 20,\n",
       " 'min_child_weight': 0.001,\n",
       " 'min_split_gain': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': -1,\n",
       " 'num_leaves': 31,\n",
       " 'objective': None,\n",
       " 'random_state': None,\n",
       " 'reg_alpha': 0.0,\n",
       " 'reg_lambda': 0.0,\n",
       " 'silent': True,\n",
       " 'subsample': 1.0,\n",
       " 'subsample_for_bin': 200000,\n",
       " 'subsample_freq': 0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lgbm.LGBMClassifier()\n",
    "model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the parameters available to us for tuning. A description of each can be found at:\n",
    "https://lightgbm.readthedocs.io/en/latest/Python-API.html#scikit-learn-api\n",
    "https://lightgbm.readthedocs.io/en/latest/Parameters.html  \n",
    "We pick some of the hyper-parameters to tune:  \n",
    "To tune:\n",
    "- boosting_type: the type of boosting method to be used. available options: ['gbdt','dart','goss','rf']\n",
    "    - we won't be including random forest since we used that as our benchmark\n",
    "- colsample_bytree (float, optional (default=1.)). we'll create options around the default value, []\n",
    "- learning_rate: used to control the growth of the trees - we need to vary this on a log scale\n",
    "- min_child_weight/min_child_samples: determining the complexity of each tree. we'll pick one to tune\n",
    "- subsample: ratio for sampling used for building trees\n",
    "- reg_alpha: L1 regularization constant  \n",
    "- reg_lambda: L2 regularization constant\n",
    "- is_unbalance: used to specify if the class of the sample is balanced or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_grid = {'boosting_type': ['gbdt', 'goss', 'dart'],\n",
    "    'num_leaves': list(range(20, 150)),\n",
    "    'learning_rate': list(np.logspace(np.log10(0.005), np.log10(0.5), base = 10, num = 1000)),\n",
    "    'min_child_samples': list(range(10, 200, 5)),\n",
    "    'reg_alpha': list(np.linspace(0, 1)),\n",
    "    'reg_lambda': list(np.linspace(0, 1)),\n",
    "    'colsample_bytree': list(np.linspace(0.5, 1, 10)),\n",
    "    'subsample': list(np.linspace(0.6, 1, 100)), \n",
    "    'subsample_for_bin':list(range(20000, 300000, 20000)),\n",
    "    'is_unbalance': [True, False]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Objective Function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the **objective function** is to evaluate the performance of the model based on the hyperparameters. It needs to take the randomly sampled hyperparameters as the input and return the score of the model.We will be using the lightgbm's cv method along with early stopping to determine the n_estimators. This method returns a dictionary containing the score of the model. We can determine the number of estimators used by measuring the length of the score list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amah\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:394: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    }
   ],
   "source": [
    "cv_results = lgbm.cv(model.get_params(),train_set,num_boost_round=100,nfold=5,metrics='auc',seed=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_func(params):\n",
    "    \n",
    "    if 'n_estimators' in params.keys():\n",
    "        del params['n_estimators']\n",
    "    \n",
    "    cv_results = lgbm.cv(params,train_set,num_boost_round=1000,\\\n",
    "                         early_stopping_rounds=100,nfold=5,metrics='auc',seed=50)\n",
    "    \n",
    "    params['n_estimators'] = len(cv_results['auc-mean'])\n",
    "    params['score'] = cv_results['auc-mean'][-1]\n",
    "    #remove keys not needed\n",
    "    del params['metric'],params['verbose']\n",
    "#     print('length of params after adding score: %s'%(len(params.keys())))\n",
    "#     print(params.keys())\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Search Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this function is to randomly sample hyper-parameters from the search domain, pass them to the objective function and then store the results into a datastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify maximum iterations\n",
    "search_iters = 10\n",
    "\n",
    "#create dataframe to stpre result\n",
    "cols = list(hyper_grid.keys()) + ['score','n_estimators']\n",
    "results = pd.DataFrame(columns=cols,index=range(search_iters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make csv file to store results\n",
    "file_name = 'random_search_resutls.csv'\n",
    "with open(file_name, 'w') as csvfile:\n",
    "    #create writer object\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(search_grid,search_iters):\n",
    "    count = 0\n",
    "    params = {}\n",
    "    best_score = 0\n",
    "    start = time.time()\n",
    "    for i in range(search_iters):\n",
    "        \n",
    "        for key in hyper_grid.keys():\n",
    "            params[key] = random.choice(hyper_grid[key]) \n",
    "\n",
    "        if params['boosting_type'] == 'goss': params['subsample'] = 1.0\n",
    "#         print('shape of params: %s'%str(len(params.keys())))\n",
    "        #call objective function with sampled params    \n",
    "        param_score = objective_func(params)\n",
    "        if param_score['score'] > best_score :\n",
    "            best_score = param_score['score'] \n",
    "        \n",
    "        #store results\n",
    "        results.loc[i] = list(param_score.values())\n",
    "        \n",
    "        print('\\rIteration:{}, Score:{:.3f}, Best Score: {:.3f}'.format(i+1,param_score['score'],best_score),end='')\n",
    "        sys.stdout.flush()\n",
    "        #store results in a file\n",
    "        with open(file_name,'a') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(list(param_score.values()))\n",
    "    end = time.time()\n",
    "    print('\\nSearch finished in %.2f minutes'%((end-start)/60))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Search for Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:1, Score:0.684, Best Score: 0.684\n",
      "Search finished in 0.22 minutes\n"
     ]
    }
   ],
   "source": [
    "search_results = random_search(hyper_grid,1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
