{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import ftextraction\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import Imputer\n",
    "import lightgbm as lgbm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from utils import plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import gc\n",
    "gc.enable()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = plotting.Visualizations()\n",
    "fe = ftextraction.Extractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocess datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load datasets\n",
    "start = time.time()\n",
    "app_train = pd.read_csv('data/application_train.csv')\n",
    "app_test = pd.read_csv('data/application_test.csv')\n",
    "buro = pd.read_csv('data/bureau.csv')\n",
    "bb = pd.read_csv('data/bureau_balance.csv')\n",
    "ccb = pd.read_csv('data/credit_card_balance.csv')\n",
    "ipay = pd.read_csv('data/installments_payments.csv')\n",
    "pos = pd.read_csv('data/POS_CASH_balance.csv')\n",
    "pa = pd.read_csv('data/previous_application.csv')\n",
    "end = time.time()\n",
    "print('Datasets loaded in: {:.2f} minutes'.format((end-start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process datasets\n",
    "start = time.time()\n",
    "# data = fe.process_datasets(app_train,app_test,buro,bb,pa,ipay,ccb,pos,fe=False,path='processed/alldata.csv')\n",
    "data_fe = fe.process_datasets(app_train,app_test,buro,bb,pa,ipay,ccb,pos,path='processed/alldata_fe.csv')\n",
    "end = time.time()\n",
    "print('Datasets processed in: {:.2f} minutes'.format((end-start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('processed/alldata.csv')\n",
    "data_fe = pd.read_csv('processed/alldata_fe.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASELINE MODELS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAINING PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_modelling(model,data):\n",
    "       \n",
    "    #preprocessing data    \n",
    "    train = data.loc[data['TARGET'].notnull(),:].copy()\n",
    "    test = data.loc[data['TARGET'].isnull(),:].copy()\n",
    "    del data\n",
    "    gc.collect()\n",
    "    # save test IDs for final submission dataframe\n",
    "    test_IDs = test['SK_ID_CURR']\n",
    "    # save target labels\n",
    "    labels = train['TARGET']\n",
    "    # drop ID columns\n",
    "    train = train.drop(columns=['SK_ID_CURR','TARGET'])\n",
    "    test = test.drop(columns=['SK_ID_CURR'])\n",
    "\n",
    "    #drop features with a high number of missing values:\n",
    "    train = fe.drop_missing_values(train,20)\n",
    "    test = fe.drop_missing_values(test,20)\n",
    "    \n",
    "    # aligining dataframes\n",
    "    train,test = train.align(test,join='inner',axis=1)\n",
    "\n",
    "    # Impute training and testing data\n",
    "    \n",
    "    imputer = Imputer(strategy='mean')\n",
    "    imputer.fit(train)\n",
    "    train = imputer.transform(train)\n",
    "    test = imputer.transform(test)\n",
    "\n",
    "    # converting to numpy array for lgbm consumptions\n",
    "    train = np.array(train)\n",
    "    test = np.array(test)\n",
    "\n",
    "    #DATA STRUCTURES TO STORE PREDICTIONS AND METRICS\n",
    "\n",
    "    #store cv predictions\n",
    "    oof_predictions = np.zeros(train.shape[0])\n",
    "    #store predictions on test dataset\n",
    "    test_preds = np.zeros(test.shape[0])\n",
    "    #store predictions on train dataset\n",
    "    train_preds = np.zeros(train.shape[0])\n",
    "    #store scores\n",
    "    scores = {}\n",
    "\n",
    "    #SPLITTING AND TRAINING \n",
    "    kfold = StratifiedKFold(n_splits=3,shuffle=True,random_state=40)\n",
    "    fold = 1\n",
    "    start = time.time()\n",
    "    print('Training started')\n",
    "    for train_i,valid_i in kfold.split(train,labels):\n",
    "        xtrain,ytrain = train[train_i],labels[train_i]\n",
    "        xvalid,yvalid = train[valid_i],labels[valid_i]\n",
    "        \n",
    "        #classifier object\n",
    "        classifier = copy.copy(model)\n",
    "        # fitting on the training set\n",
    "        classifier.fit(xtrain,ytrain)\n",
    " \n",
    "        # storing out of fold predictions:\n",
    "        oof_predictions[valid_i] = classifier.predict_proba(xvalid)[:, 1]\n",
    "\n",
    "        # storing test set predictions:\n",
    "        test_preds += classifier.predict_proba(test)[:, 1] /kfold.n_splits\n",
    "\n",
    "        #storing training set predictions\n",
    "        train_preds[train_i] = classifier.predict_proba(xtrain)[:, 1]\n",
    "        \n",
    "        #false and true positive rates for plotting roc curve\n",
    "        fpr,tpr,threshold = roc_curve(yvalid,oof_predictions[valid_i])\n",
    "        print('Fold %d done.'%fold)\n",
    "        fold += 1\n",
    "        # freeing up memory\n",
    "        del xtrain,ytrain,xvalid,yvalid,classifier\n",
    "        gc.collect()\n",
    "        \n",
    "    end = time.time()    \n",
    "    print('Training done in {:.2f} minutes'.format((end-start)/60))    \n",
    "    #SCORES\n",
    "    \n",
    "    training_score = roc_auc_score(labels,train_preds)\n",
    "    cv = roc_auc_score(labels,oof_predictions)\n",
    "    \n",
    "    fpr,tpr,threshold = roc_curve(labels,oof_predictions)\n",
    "    \n",
    "    scores['training score'] = [training_score]\n",
    "    scores['cv score'] = [cv]\n",
    "    scores = pd.DataFrame.from_dict(scores)\n",
    "    print('Overall CV Score: %.4f' %cv)\n",
    "    # submission dataframe:\n",
    "    submission = pd.DataFrame({'SK_ID_CURR': test_IDs, 'TARGET': test_preds})\n",
    "\n",
    "    return submission, scores,fpr,tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline models\n",
    "rf = RandomForestClassifier()\n",
    "lr = LogisticRegression()\n",
    "xgb = XGBClassifier()\n",
    "classifiers = [rf,lr,xgb]\n",
    "model_scores = []\n",
    "model_predictions = []\n",
    "model_fprs = []\n",
    "model_tprs = []\n",
    "\n",
    "for classifier in classifiers:\n",
    "    print(f'Training {classifier.__class__.__name__} model')\n",
    "    submission,scores,fpr,tpr = baseline_modelling(classifier,data)\n",
    "    model_scores.append(scores)\n",
    "    model_predictions.append(submission)\n",
    "    model_fprs.append(fpr)\n",
    "    model_tprs.append(tpr)\n",
    "    print('Training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model scores\n",
    "clf_names = ['Random Forest','Logistic Regression','XGBoost']\n",
    "all_scores = pd.concat(model_scores)\n",
    "all_scores.index = clf_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>training score</th>\n",
       "      <th>cv score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.999753</td>\n",
       "      <td>0.629552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.635356</td>\n",
       "      <td>0.633084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.778391</td>\n",
       "      <td>0.765616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     training score  cv score\n",
       "Random Forest              0.999753  0.629552\n",
       "Logistic Regression        0.635356  0.633084\n",
       "XGBoost                    0.778391  0.765616"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions[2].to_csv('submissions/xgb_baseline.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,5))\n",
    "for i in range(len(model_fprs)):\n",
    "    base_roc = auc(model_fprs[i],model_tprs[i])\n",
    "    plt.plot(model_fprs[i],model_tprs[i],label='{} ROC: {:.4f}'.format(clf_names[i],base_roc))\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',label='Naive', alpha=0.5)\n",
    "plt.title('ROC Curves for Baseline Models')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "# plt.show()\n",
    "plt.savefig('baseline_roc.png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOLUTION MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data,folds,iterations,exclude=[]):\n",
    "       \n",
    "    #preprocessing data    \n",
    "    train = data.loc[data['TARGET'].notnull(),:].copy()\n",
    "    test = data.loc[data['TARGET'].isnull(),:].copy()\n",
    "    del data\n",
    "    gc.collect()\n",
    "\n",
    "    # save test IDs for final submission dataframe\n",
    "    test_IDs = test['SK_ID_CURR']\n",
    "    # save target labels\n",
    "    labels = train['TARGET']\n",
    "    # drop ID columns\n",
    "    train = train.drop(columns=['SK_ID_CURR','TARGET'])\n",
    "    test = test.drop(columns=['SK_ID_CURR'])\n",
    "\n",
    "\n",
    "    # drop features with no importance\n",
    "    train.drop(columns=exclude,inplace=True)\n",
    "    test.drop(columns=exclude,inplace=True)\n",
    "\n",
    "    # aligining dataframes\n",
    "    train,test = train.align(test,join='inner',axis=1)\n",
    "\n",
    "    # for storing feature importances\n",
    "    features = list(train.columns)\n",
    "    ft_importances = np.zeros(len(features))\n",
    "\n",
    "    # converting to numpy array for lgbm consumptions\n",
    "    train = np.array(train)\n",
    "    test = np.array(test)\n",
    "\n",
    "    #DATA STRUCTURES TO STORE PREDICTIONS AND METRICS\n",
    "\n",
    "    #store cv predictions\n",
    "    oof_predictions = np.zeros(train.shape[0])\n",
    "    oof_predabs = np.zeros(train.shape[0])\n",
    "\n",
    "    #store predictions on test dataset\n",
    "    test_preds = np.zeros(test.shape[0])\n",
    "    #store predictions on train dataset\n",
    "    train_preds = np.zeros(train.shape[0])\n",
    "    #store scores\n",
    "    scores = {}\n",
    "\n",
    "    #SPLITTING AND TRAINING \n",
    "    kfold = StratifiedKFold(n_splits=folds,shuffle=True,random_state=40)\n",
    "    fold = 1\n",
    "    start = time.time()\n",
    "    fprs = {}\n",
    "    tprs = {}\n",
    "    aucs = []\n",
    "    models = []\n",
    "    print('Training started')\n",
    "    for train_i,valid_i in kfold.split(train,labels):\n",
    "        xtrain,ytrain = train[train_i],labels[train_i]\n",
    "        xvalid,yvalid = train[valid_i],labels[valid_i]\n",
    "    \n",
    "        # creating the classifier \n",
    "        clf = lgbm.LGBMClassifier(boosting_type='gbdt',num_leaves=40,\n",
    "                                 learning_rate=0.02,reg_alpha=1,\n",
    "                                 min_child_samples=45,reg_lambda=0.142857,\n",
    "                                 colsample_bytree=0.5,subsample=0.915152,\n",
    "                                 is_unbalance=True,\n",
    "                                 n_estimators=10000,random_state=50)\n",
    "\n",
    "        # fitting on the training set\n",
    "        clf.fit(xtrain,ytrain,eval_set=[(xtrain, ytrain), (xvalid, yvalid)],eval_metric ='auc',\n",
    "                verbose= 100, early_stopping_rounds= iterations,eval_names = ['train','valid'])\n",
    "\n",
    "        #best iteration\n",
    "        best_iter = clf.best_iteration_\n",
    "        \n",
    "        # storing out of fold predictions:\n",
    "        oof_predictions[valid_i] = clf.predict_proba(xvalid,num_iteration=best_iter)[:, 1]\n",
    "        oof_predabs[valid_i] = clf.predict_proba(xvalid,num_iteration=best_iter)[:, 1]\n",
    "\n",
    "        # storing test set predictions:\n",
    "        test_preds += clf.predict_proba(test,num_iteration=best_iter)[:, 1] /kfold.n_splits\n",
    "\n",
    "        #stortin training set predictions\n",
    "        train_preds[train_i] = clf.predict_proba(xtrain,num_iteration=best_iter)[:, 1]\n",
    "\n",
    "        # storing feature importances            \n",
    "        ft_importances += clf.feature_importances_ /kfold.n_splits\n",
    "        \n",
    "        #false and true positive rates for plotting roc curve\n",
    "        fpr,tpr,threshold = roc_curve(yvalid,oof_predictions[valid_i])\n",
    "        fprs[f'fold_{fold}'] = fpr\n",
    "        tprs[f'fold_{fold}'] = tpr\n",
    "        rauc_scr = auc(fpr,tpr)\n",
    "        aucs.append(rauc_scr)\n",
    "        print('Fold %d done.'%fold)\n",
    "        fold += 1\n",
    "        #store model\n",
    "        models.append(clf)\n",
    "        # freeing up memory\n",
    "        del xtrain,ytrain,xvalid,yvalid,clf\n",
    "        gc.collect()\n",
    "        \n",
    "    end = time.time()    \n",
    "    print('Training done in {:.2f} minutes'.format((end-start)/60))    \n",
    "    #SCORES\n",
    "\n",
    "    feature_importances = pd.DataFrame({'features':features,'importance':ft_importances})\n",
    "    \n",
    "    training_score = roc_auc_score(labels,train_preds)\n",
    "    cv = roc_auc_score(labels,oof_predictions)\n",
    "    \n",
    "    fpr,tpr,threshold = roc_curve(labels,oof_predictions)\n",
    "    fprs['OCV'] = fpr\n",
    "    tprs['OCV'] = tpr\n",
    "    aucs.append(cv) \n",
    "    \n",
    "    scores['training score'] = [training_score]\n",
    "    scores['cv score'] = [cv]\n",
    "    scores = pd.DataFrame.from_dict(scores)\n",
    "    print('Overall CV Score: %.4f' %cv)\n",
    "    # submission dataframe:\n",
    "    submission = pd.DataFrame({'SK_ID_CURR': test_IDs, 'TARGET': test_preds})\n",
    "\n",
    "    return models,submission, scores, oof_predabs, feature_importances,fprs,tprs,aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature selection from previous runs\n",
    "fi = pd.read_csv('fi_lgbm_V0_fe.csv')\n",
    "no_imp=list(fi.loc[fi['importance']==0,'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdls,fsubmission, fscores, oof_predabs, fi_final,ffprs,ftprs,faucs = model(data_fe,10,100,exclude=no_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Report "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC Curves of all Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbmV0(data,folds,exclude=[]):\n",
    "       \n",
    "    #preprocessing data    \n",
    "    train = data.loc[data['TARGET'].notnull(),:].copy()\n",
    "    test = data.loc[data['TARGET'].isnull(),:].copy()\n",
    "    del data\n",
    "    gc.collect()\n",
    "\n",
    "    # save test IDs for final submission dataframe\n",
    "    test_IDs = test['SK_ID_CURR']\n",
    "    # save target labels\n",
    "    labels = train['TARGET']\n",
    "    # drop ID columns\n",
    "    train = train.drop(columns=['SK_ID_CURR','TARGET'])\n",
    "    test = test.drop(columns=['SK_ID_CURR'])\n",
    "\n",
    "\n",
    "    # drop features with no importance\n",
    "    train.drop(columns=exclude,inplace=True)\n",
    "    test.drop(columns=exclude,inplace=True)\n",
    "\n",
    "    # aligining dataframes\n",
    "    train,test = train.align(test,join='inner',axis=1)\n",
    "\n",
    "    # for storing feature importances\n",
    "    features = list(train.columns)\n",
    "    ft_importances = np.zeros(len(features))\n",
    "\n",
    "    # converting to numpy array for lgbm consumptions\n",
    "    train = np.array(train)\n",
    "    test = np.array(test)\n",
    "\n",
    "    #DATA STRUCTURES TO STORE PREDICTIONS AND METRICS\n",
    "\n",
    "    #store cv predictions\n",
    "    oof_predictions = np.zeros(train.shape[0])\n",
    "    #store predictions on test dataset\n",
    "    test_preds = np.zeros(test.shape[0])\n",
    "    #store predictions on train dataset\n",
    "    train_preds = np.zeros(train.shape[0])\n",
    "    #store scores\n",
    "    scores = {}\n",
    "\n",
    "    #SPLITTING AND TRAINING \n",
    "    kfold = StratifiedKFold(n_splits=folds,shuffle=True,random_state=40)\n",
    "    fold = 1\n",
    "    start = time.time()\n",
    "    print('Training started')\n",
    "    for train_i,valid_i in kfold.split(train,labels):\n",
    "        xtrain,ytrain = train[train_i],labels[train_i]\n",
    "        xvalid,yvalid = train[valid_i],labels[valid_i]\n",
    "        \n",
    "        # creating the classifier \n",
    "        clf = lgbm.LGBMClassifier()\n",
    "\n",
    "        # fitting on the training set, early stopping using validation set\n",
    "        clf.fit(xtrain,ytrain,verbose= 200)\n",
    "\n",
    "        # storing out of fold predictions:\n",
    "        oof_predictions[valid_i] = clf.predict_proba(xvalid)[:, 1]\n",
    "\n",
    "        # storing test set predictions:\n",
    "        test_preds += clf.predict_proba(test)[:, 1] /kfold.n_splits\n",
    "\n",
    "        #stortin training set predictions\n",
    "        train_preds[train_i] = clf.predict_proba(xtrain)[:, 1]\n",
    "\n",
    "        # storing feature importances            \n",
    "        ft_importances += clf.feature_importances_ /kfold.n_splits\n",
    "        print('Fold %d done.'%fold)\n",
    "        fold += 1\n",
    "        # freeing up memory\n",
    "        del xtrain,ytrain,xvalid,yvalid,clf\n",
    "        gc.collect()\n",
    "        \n",
    "    end = time.time()    \n",
    "    print('Training done in {:.2f} minutes'.format((end-start)/60))    \n",
    "    #SCORES\n",
    "\n",
    "    feature_importances = pd.DataFrame({'features':features,'importance':ft_importances})\n",
    "    \n",
    "    training_score = roc_auc_score(labels,train_preds)\n",
    "    cv = roc_auc_score(labels,oof_predictions)\n",
    "    scores['training score'] = [training_score]\n",
    "    scores['cv score'] = [cv]\n",
    "    scores = pd.DataFrame.from_dict(scores)\n",
    "    print('Overall CV Score: %.4f' %cv)\n",
    "    # submission dataframe:\n",
    "    submission = pd.DataFrame({'SK_ID_CURR': test_IDs, 'TARGET': test_preds})\n",
    "\n",
    "    return submission, scores, feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without engineered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_lgbmV0, scores_lgbmV0, fi_lgbm_V0 = lgbmV0(data,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With engineered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_lgbmV0_fe, scores_lgbmV0_fe, fi_lgbm_V0_fe = lgbmV0(data_fe,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs.plot_fi(fi_lgbm_V0_fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_lgbm_V0_fe = pd.read_csv('fi_lgbm_V0_fe.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'boosting_type':'gbdt','num_leaves':40,\n",
    "           'learning_rate':0.02,'min_child_samples':45,\n",
    "           'reg_alpha':1,'reg_lambda':0.142857,\n",
    "           'colsample_bytree':0.5,'subsample':0.915152,\n",
    "           'is_unbalance':True,'n_estimators':10000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbmV1(data,folds,params,iterations,exclude=[]):\n",
    "       \n",
    "    #preprocessing data    \n",
    "    train = data.loc[data['TARGET'].notnull(),:].copy()\n",
    "    test = data.loc[data['TARGET'].isnull(),:].copy()\n",
    "    del data\n",
    "    gc.collect()\n",
    "\n",
    "    # save test IDs for final submission dataframe\n",
    "    test_IDs = test['SK_ID_CURR']\n",
    "    # save target labels\n",
    "    labels = train['TARGET']\n",
    "    # drop ID columns\n",
    "    train = train.drop(columns=['SK_ID_CURR','TARGET'])\n",
    "    test = test.drop(columns=['SK_ID_CURR'])\n",
    "\n",
    "\n",
    "    # drop features with no importance\n",
    "    train.drop(columns=exclude,inplace=True)\n",
    "    test.drop(columns=exclude,inplace=True)\n",
    "\n",
    "    # aligining dataframes\n",
    "    train,test = train.align(test,join='inner',axis=1)\n",
    "\n",
    "    # for storing feature importances\n",
    "    features = list(train.columns)\n",
    "    ft_importances = np.zeros(len(features))\n",
    "\n",
    "    # converting to numpy array for lgbm consumptions\n",
    "    train = np.array(train)\n",
    "    test = np.array(test)\n",
    "\n",
    "    #DATA STRUCTURES TO STORE PREDICTIONS AND METRICS\n",
    "\n",
    "    #store cv predictions\n",
    "    oof_predictions = np.zeros(train.shape[0])\n",
    "    #store predictions on test dataset\n",
    "    test_preds = np.zeros(test.shape[0])\n",
    "    #store predictions on train dataset\n",
    "    train_preds = np.zeros(train.shape[0])\n",
    "    #store scores\n",
    "    scores = {}\n",
    "\n",
    "    #SPLITTING AND TRAINING \n",
    "    kfold = StratifiedKFold(n_splits=folds,shuffle=True,random_state=40)\n",
    "    fold = 1\n",
    "    start = time.time()\n",
    "    fprs = {}\n",
    "    tprs = {}\n",
    "    aucs = []\n",
    "    models = []\n",
    "    print('Training started')\n",
    "    for train_i,valid_i in kfold.split(train,labels):\n",
    "        xtrain,ytrain = train[train_i],labels[train_i]\n",
    "        xvalid,yvalid = train[valid_i],labels[valid_i]\n",
    "        \n",
    "        # creating the classifier \n",
    "        clf = lgbm.LGBMClassifier(boosting_type=params['boosting_type'],num_leaves=params['num_leaves'],\n",
    "                                 learning_rate=params['learning_rate'],reg_alpha=params['reg_alpha'],\n",
    "                                 min_child_samples=params['min_child_samples'],reg_lambda=params['reg_lambda'],\n",
    "                                 colsample_bytree=params['colsample_bytree'],subsample=params['subsample'],\n",
    "                                 is_unbalance=params['is_unbalance'],\n",
    "                                 n_estimators=params['n_estimators'])\n",
    "\n",
    "        # fitting on the training set\n",
    "        clf.fit(xtrain,ytrain,eval_set=[(xtrain, ytrain), (xvalid, yvalid)],eval_metric ='auc',\n",
    "                verbose= 200, early_stopping_rounds= iterations,eval_names = ['train','valid'])\n",
    "\n",
    "        #best iteration\n",
    "        best_iter = clf.best_iteration_\n",
    "        \n",
    "        # storing out of fold predictions:\n",
    "        oof_predictions[valid_i] = clf.predict_proba(xvalid,num_iteration=best_iter)[:, 1]\n",
    "\n",
    "        # storing test set predictions:\n",
    "        test_preds += clf.predict_proba(test,num_iteration=best_iter)[:, 1] /kfold.n_splits\n",
    "\n",
    "        #stortin training set predictions\n",
    "        train_preds[train_i] = clf.predict_proba(xtrain,num_iteration=best_iter)[:, 1]\n",
    "\n",
    "        # storing feature importances            \n",
    "        ft_importances += clf.feature_importances_ /kfold.n_splits\n",
    "        \n",
    "        #false and true positive rates for plotting roc curve\n",
    "        fpr,tpr,threshold = roc_curve(yvalid,oof_predictions[valid_i])\n",
    "        fprs[f'fold_{fold}'] = fpr\n",
    "        tprs[f'fold_{fold}'] = tpr\n",
    "        rauc_scr = auc(fpr,tpr)\n",
    "        aucs.append(rauc_scr)\n",
    "        print('Fold %d done.'%fold)\n",
    "        fold += 1\n",
    "        #store model\n",
    "        models.append(clf)\n",
    "        # freeing up memory\n",
    "        del xtrain,ytrain,xvalid,yvalid,clf\n",
    "        gc.collect()\n",
    "        \n",
    "    end = time.time()    \n",
    "    print('Training done in {:.2f} minutes'.format((end-start)/60))    \n",
    "    #SCORES\n",
    "\n",
    "    feature_importances = pd.DataFrame({'features':features,'importance':ft_importances})\n",
    "    \n",
    "    training_score = roc_auc_score(labels,train_preds)\n",
    "    cv = roc_auc_score(labels,oof_predictions)\n",
    "    \n",
    "    fpr,tpr,threshold = roc_curve(labels,oof_predictions)\n",
    "    fprs['OCV'] = fpr\n",
    "    tprs['OCV'] = tpr\n",
    "    aucs.append(cv) \n",
    "    \n",
    "    scores['training score'] = [training_score]\n",
    "    scores['cv score'] = [cv]\n",
    "    scores = pd.DataFrame.from_dict(scores)\n",
    "    print('Overall CV Score: %.4f' %cv)\n",
    "    # submission dataframe:\n",
    "    submission = pd.DataFrame({'SK_ID_CURR': test_IDs, 'TARGET': test_preds})\n",
    "\n",
    "    return models,submission, scores, feature_importances,fprs,tprs,aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_lgbmV1, scores_lgbmV1, fi_lgbm_V1,sfprs,stprs,saucs = lgbmV1(data_fe,5,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_lgbmV1.to_csv('submission_lgbmV1.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelling without no-importance features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = pd.read_csv('fi_lgbm_V0_fe.csv')\n",
    "to_drop=list(fi.loc[fi['importance']==0,'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_lgbmV1_s, scores_lgbmV1_s, fi_lgbm_V1_s,fprs,tprs,aucs = lgbmV1(data_fe,5,params,exclude=to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_lgbmV1_s.to_csv('submission_lgbmV1_s.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plotting ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,5))\n",
    "all_folds = list(fprs.keys())\n",
    "for i in range(5):\n",
    "    fold = all_folds[i]\n",
    "    plt.plot(fprs[fold],tprs[fold]\n",
    "         ,alpha=0.3,label='{} ROC AUC:{:.4f}'.format(fold,aucs[i]))\n",
    "\n",
    "plt.plot(fprs['OCV'], tprs['OCV'],\n",
    "         label='Mean ROC AUC:{:.4f}'.format(aucs[-1]),color='g')\n",
    "\n",
    "plt.plot(fpr, tpr,\n",
    "         label='Baseline: {:.4f}'.format(base_roc),color='c')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Naive', alpha=0.5)\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINAL MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, submission_final, scores_final, fi_final,ffprs,ftprs,faucs = lgbmV1(data_fe,5,params,100,exclude=to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model[-1].get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sensitivity Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort feature importance\n",
    "fi.sort_values('importance',ascending=False,inplace=True)\n",
    "fi.reset_index(inplace=True,drop =True)\n",
    "#keep features accouting for only 90% of importnace\n",
    "fi['fi_cumsum'] = fi['importance'].cumsum()\n",
    "#exclude features beyond 90% importance\n",
    "drop_10 = list(fi.loc[fi['fi_cumsum']>0.9*fi['importance'].sum(),'features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelling with 90% importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sublgbm90, scores_lgbm90, fi_lgbm90,fprs90,tprs90,aucs90 = lgbmV1(data_fe,5,params,exclude=drop_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sublgbm90.to_csv('sublgbm90.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelling without top 3 important features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_top3 = list(fi.loc[:2,'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sublgbmt3, scores_lgbmt3, fi_lgbmt3,fprst3,tprst3,aucst3 = lgbmV1(data_fe,5,params,exclude=drop_top3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
